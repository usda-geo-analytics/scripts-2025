{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEREIN, AS ALWAYS, BEGIN TEH ALGORITHMS!!\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from arcgis.gis import GIS\n",
    "gis = GIS(\"home\")\n",
    "\n",
    "# Not sure if I will need additional imports...??\n",
    "from arcgis.features import FeatureLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the master layer to which we will be appending records...\n",
    "# will we want one just for drought and one for ALL, as reflected in the Excel files?...\n",
    "\n",
    "# Drought layer\n",
    "#drought_lyr = gis.content.get(\"id_goes_here\").layers[0]\n",
    "\n",
    "# All disaster layer?\n",
    "#all_lyr = gis.content.get(\"id_goes_here\").layers[1]\n",
    "\n",
    "#processed = gis.content.get(\"id_goes_here\").layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Excel input folder\n",
    "in_dir = r'C:\\Users\\misti.wudtke\\OneDrive - USDA\\PROJECTS\\FY25_Q2_FSA_DisasterDashboard_Alanda\\Input_Excels\\Script_Input'\n",
    "\n",
    "# Names to apply to columns for consistency (in case of input inconsistency)\n",
    "col_names = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\misti.wudtke\\\\OneDrive - USDA\\\\PROJECTS\\\\FY25_Q2_FSA_DisasterDashboard_Alanda\\\\Input_Excels\\\\Script_Input\\\\All_Crop_Counties_032724.xlsx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xls = [f'{in_dir}{os.sep}{f}' for f in os.listdir(in_dir) if f.endswith('.xlsx')]\n",
    "\n",
    "if not xls:\n",
    "\n",
    "    print(\"No files?? Did you forget to add, dumbass??\\nExit stage left!!\\n\")\n",
    "    sys.exit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file All_Crop_Counties_032724.xlsx:\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing sheet All Drought:\n",
      "------------------------------\n",
      "\n",
      "Datatypes:\n",
      "fips     int64\n",
      "desig    int64\n",
      "dtype: object\n",
      "\n",
      "No dups found! Happily forward!\n",
      "\n",
      "Dataframe preview:\n",
      "       desig\n",
      "fips        \n",
      "01007      1\n",
      "01009      1\n",
      "01015      1\n",
      "01019      1\n",
      "01021      1\n",
      "\n",
      "Processing sheet All_Crop_032724:\n",
      "------------------------------\n",
      "\n",
      "Datatypes:\n",
      "fips     int64\n",
      "desig    int64\n",
      "dtype: object\n",
      "\n",
      "No dups found! Happily forward!\n",
      "\n",
      "Dataframe preview:\n",
      "       desig\n",
      "fips        \n",
      "01007      1\n",
      "01009      1\n",
      "01015      1\n",
      "01019      1\n",
      "01021      1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through Excel files...in most cases there should only be 1 file,\n",
    "# But making flexible just in case of...backlog or whatever, I dunno...\n",
    "for exl in exls:\n",
    "\n",
    "    # Gru: WE READ THE EXCEL\n",
    "    print(f\"\\nProcessing file {os.path.basename(exl)}:\")\n",
    "    print(\"--------------------------------------------------\\n\")\n",
    "\n",
    "    for sheet in pd.ExcelFile(exl).sheet_names:\n",
    "        \n",
    "        print(f\"Processing sheet {sheet}:\")\n",
    "        print(\"------------------------------\\n\")\n",
    "\n",
    "        # NOTE: This import of the data ASSUMES THE FIPS \n",
    "        # AND DESIGNATIONS ARE FOUND IN COLUMNS A AND D!!!\n",
    "        # If this changes, the arg for usecols must be adjusted\n",
    "        df = pd.read_excel(exl, sheet_name=sheet, usecols=[0, 3])\n",
    "\n",
    "        # Names to apply to columns for consistency (in case of input inconsistency)\n",
    "        df.columns = ['fips', 'desig']\n",
    "\n",
    "        print(f'Datatypes:\\n{df.dtypes}\\n')\n",
    "\n",
    "        df['fips'] = df['fips'].astype(str).str.zfill(5)\n",
    "\n",
    "        dups = df[df['fips'].duplicated()]\n",
    "\n",
    "        if dups.empty:\n",
    "            print(\"No dups found! Happily forward!\\n\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Duplicate FIPS values found in sheet {sheet} of file {os.path.basename(exl)}!\\n\")\n",
    "            print(f\"Dup fips: {dups['fips'].tolist()}\\n\")\n",
    "            print(\"Figure you don't want them there; exiting now so you can fix; you're welcome!\\n\")\n",
    "            sys.exit\n",
    "\n",
    "        df = df.set_index('fips')\n",
    "\n",
    "        print(f'Dataframe preview:\\n{df.head()}\\n')\n",
    "\n",
    "        ########## ########## ########## ########## ########## ########## ########## ########## \n",
    "\n",
    "        # Gru: WE PROCESS THE EXCEL\n",
    "        \n",
    "\n",
    "\n",
    "        ########## ########## ########## ########## ########## ########## ########## ########## \n",
    "\n",
    "        # Gru: WE APPEND THE EXCEL\n",
    "\n",
    "        # Well OK not so fast we'll need to make sure these records haven't already been processed\n",
    "        # How are we going to do that? I could do some fancy checksum bs...but I don't think it's worth the trouble\n",
    "        # I will just tell Alanda \"don't change the input file name\" and we'll parse that and check for it...\n",
    "        # could put the names in a little table included in the feature service; sure why not?\n",
    "\n",
    "\n",
    "        # No really this is literally all that's happening here\n",
    "        # I think that's pretty much the end of the algorithm"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
